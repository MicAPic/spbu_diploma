\section{Обзор методов и технологий тестирования игр}

\subsection{Обзор стандартных методов}
Существует ряд различий между тестированием игр и тестированием традиционного программного обеспечения \cite{santos2018computer}. В первую очередь, геймдев характеризуется превалированием ручного тестирования. Это объясняется большим количеством аспектов, требующих оценку в процессе контроля качества игрового продукта. Тестировщики проверяют геймплей, логичность и наблюдаемость процессов, прогрессивность мышления, а также всесторонне тестируют функционал игр \cite{aleem2016critical}. 

Кроме того, ключевым различием между игровой и неигровой разработками является неготовность специалистов в геймдеве использовать методы автоматизированного тестирования \cite{murphy2014cowboys}. Это вызвано рядом причин:
\begin{itemize}
	\item[--] для игр сложно писать автоматизированные тесты, в т.ч. потому что очень трудно отделить пользовательский интерфейс (\textit{user interface}, \textit{UI}) от остальной части игры;
	\item[--] большое пространство состояний, характерное для игр, сложнее исследовать;
	\item[--] автоматизированное тестирование неустойчиво к частым изменениям требований и дизайна игры.
\end{itemize}

\subsection{Обзор и сравнительный анализ существующих RL-библиотек для задач геймдева}
В ходе исследовательской работы были выделены четыре библиотеки для обучения с подкреплением в разработке игр. Сравнение данных библиотек приводится в Таблице~\ref{tab:libraries}.

%TODO: Добавить ссылки на библиотеки
\begin{table}[ht]
	\centering
	\caption{Cравнительный анализ существующих библиотек для обучения с подкреплением}
	\begin{adjustbox}{width=1\textwidth}
	\small
	\begin{tabular}{ l l l l l }
		\hline
		Название & \makecell[l]{Поддерживаемые\\движки} & \makecell[l]{Поддерживаемые\\платформы для\\тренировки сети} & \makecell[l]{Открытость\\кода} & \makecell[l]{Встроенные\\одноагентные\\алгоритмы} \\
		\hline
		 \makecell[l]{Unity\\ML-Agents} & \makecell[l]{\textbf{Unity 2022.3}\\\textbf{и выше}} & \makecell[l]{Windows,\\\textbf{macOS},\\Linux} & \textbf{Открыт} & PPO, SAC\\
		 \makecell[l]{AILIVE} & \makecell[l]{\textbf{Unity 2018.4}\\\textbf{и выше}} & \makecell[l]{Windows} & Закрыт & --\\
		 \makecell[l]{Learning\\Agents} & \makecell[l]{Unreal Engine\\5.3 и выше} & \makecell[l]{Windows} & \textbf{Открыт} & PPO\\
		 \makecell[l]{MindMaker} & \makecell[l]{Unreal Engine\\4.27, 5.0--5.1} & \makecell[l]{Windows} & \textbf{Открыт} & \makecell[l]{A2C, ACER,\\ACKTR,\\DQN, PPO,\\SAC, TD3,\\TRPO,\\DDPG}\\
		 \makecell[l]{Godot\\RL Agents} & \makecell[l]{Godot 4.0\\и выше} & \makecell[l]{Windows,\\\textbf{macOS},\\Linux} & \textbf{Открыт} & \makecell[l]{\textbf{более 20}\\\textbf{алгоритмов}}\\
		\hline
	\end{tabular}
	\end{adjustbox}
	\label{tab:libraries}
\end{table}

Дальнейшая работа будет проводиться при помощи библиотеки ML-Agents. Данный выбор обусловлен поддержкой платформы macOS и движка Unity, а также богатством документации и материалов, касающихся выбранной библиотеки.

\subsection{Обзор нейросетевых подходов тестирования игрового процесса}
Обучение с подкреплением "--- способ машинного обучения, при котором система обучается, взаимодействуя с некоторой средой, пытаясь максимизировать значение награды. Обучаемый агент не знает заранее, какие действия ему следует предпринять. Путем исследования среды он должен сам определить, какие из них принесут наибольшую пользу. В наиболее интересных и сложных случаях действия могут повлиять не только на получение немедленной награды, но и на следующую ситуацию, а через нее "--- на все последующие. Эти две характеристики "--- поиск методом проб и ошибок и отложенное вознаграждение "--- являются двумя наиболее важными отличительными особенностями обучения с подкреплением \cite{sutton2018reinforcement}.

Условно выделяются три метода обучения игровых агентов с помощью RL: обучение по пикселям, по компактным представлениям состояний, а также обучение на основе моделирования игрового процесса (через мультиагентное обучение с подкреплением) \cite{politowski2021survey}.
\begin{itemize}
	\item[--] метод \textit{обучения по пикселям} использует выходные видеоданные игры. Оно хорошо показало себя в играх для консоли \textit{Atari 2600}, разрешение которой составляло \(210 \times 160\) пикселей \cite{mnih2013playing}. Использование такого подхода в современных играх, ориентированных на более высокие разрешения, не представляется целесообразным;
	\item[--] обучение игровых агентов на основе \textit{компактного представления состояний} (таких как положение игрока, расстояние до соседних объектов или до пункта назначения и т.д.) применяется для минимизации информации о пространстве состояний на каждом временном шаге \cite{borovikov2019winning};
	\item[--] метод на основе \textit{моделирования игрового процесса} для многопользовательских игр ползволяет тестировать игровой процесс путем самостоятельной игры игровых агентов. Данная методология лежит в основе программ \textit{AlphaZero} и \textit{AlphaGo Zero}, разработанных для игры в шахматы, го и сёги \cite{silver2017mastering}.
\end{itemize}

Описанные выше методологии обучения с подкреплением дает нам следующие преимущества \cite{bergdahl2020augmenting}:
\begin{itemize}
	\item[--] RL-агенты способны обучаться на основе взаимодействия с игровой средой. В результате поведение агентов становится более похожим на поведение игроков-людей, что повышает вероятность обнаружения багов и ошибок, связанных с устройством самой среды;
	\item[--] нейросетевой агент более устойчив к изменениям и обновлениям игровой среды: он может быть переобучен или доработан с минимальными изменениями;
	\item[--] поведение агента может контролироваться путем изменения получаемой награды. Таким образом можно симулировать поведение игроков с разным игровым опытом и стилем игры.
\end{itemize}

\subsection{Обзор метрик оценки качества}
Алгоритмы обучения с подкреплением, особенно алгоритмы глубокого RL, как правило, сильно различаются по производительности и очень чувствительны к целому ряду различных факторов, включая среду, детали реализации и выбор гиперпараметов \cite{henderson2018deep}.

Чаще всего алгоритмы оцениваются по представленным графикам или таблицам со средней наградой, достигавшейся агентом в каждый эпизод или временной шаг. В сочетании с доверительными интервалами этого может быть достаточно для выбора того или иного метода при достаточно большом количестве испытаний. И наоборот: пересечение доверительных интервалов может говорить о том, что разница между алгоритмами не является значительной.

Мы можем сравнивать не только процесс тренировки различных алгоритмов RL, но и то, как они справляются с задачей тестирования игрового процесса путем сравнивания выборочного среднего, медианы и среднего квадратического отклонения числа найденных багов, участков с низким числом кадров в секунду (\textit{frames per second}, \textit{FPS}) или \textit{out of bounds} (локаций, находящихся за пределами предполагаемых границ игры) для \(n\) запусков каждой полученной модели \cite{tufano2022using}.